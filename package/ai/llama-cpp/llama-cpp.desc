[COPY] --- T2-COPYRIGHT-BEGIN ---
[COPY] t2/package/*/llama-cpp/llama-cpp.desc
[COPY] Copyright (C) 2025 The T2 SDE Project
[COPY] SPDX-License-Identifier: GPL-2.0
[COPY] --- T2-COPYRIGHT-END ---

[I] LLM inference in C/C++

[T] The main goal of llama.cpp is to enable LLM inference with minimal setup
[T] and state-of-the-art performance on a wide range of hardware - locally and
[T] in the cloud.

[U] https://github.com/ggerganov/llama.cpp

[A] llama-cpp Authors
[M] The T2 Project <t2@t2-project.org>

[C] extra/development
[F] OBJDIR

[E] opt hip-rocclr hipblas hipblas-common hipcc rocblas rocm-device-libs rocm-llvm
[E] opt vulkan-headers shaderc

[V] b6517
[L] MIT
[S] Stable
[P] X -----5---9 700.000

[D] 8c66e9ae767bb32c309ca2c6c02c8472f6ec2d29b3711fa6a8b9571f llama-cpp-b6517.tar.gz git+https://github.com/ggerganov/llama.cpp.git b6517

[ $prefix_auto = 1 ] && prefix=opt/llama-cpp && set_confopt

pkginstalled curl && var_append cmakeopt ' ' -DLLAMA_CURL=ON

if pkginstalled opencl-loader && pkginstalled opencl-headers; then
	var_append cmakeopt ' ' "-DGGML_OPENCL=ON -DGGML_OPENCL_USE_ADRENO_KERNELS=OFF"
fi

if pkginstalled hipcc; then
	export HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)"
	var_append cmakeopt ' ' "-DGGML_HIP=ON -DAMDGPU_TARGETS='$SDECFG_PKG_ROCM_ARCHS'"
fi

pkginstalled vulkan-headers && var_append cmakeopt ' ' -DGGML_VULKAN=ON

#hook_add postmake 5 "cp -rvf ../{models,scripts,prompts,examples,docs} $root/$prefix/"
