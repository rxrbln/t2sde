diff -Npruw -x CVS -x vssver.scc -x powerpc -x sync_file_range.c libc/sysdeps/x86_64/bzero.S libc/sysdeps/x86_64/bzero.S
--- libc/sysdeps/x86_64/bzero.S	2002-08-31 12:30:07.000000000 -0500
+++ libc/sysdeps/x86_64/bzero.S	2006-05-05 15:23:27.884691000 -0500
@@ -1,3 +1,5 @@
+#define USE_AS_BZERO
 #define memset __bzero
 #include <sysdeps/x86_64/memset.S>
+
 weak_alias (__bzero, bzero)
diff -Npruw -x CVS -x vssver.scc -x powerpc -x sync_file_range.c libc/sysdeps/x86_64/memcmp.S libc/sysdeps/x86_64/memcmp.S
--- libc/sysdeps/x86_64/memcmp.S	1969-12-31 18:00:00.000000000 -0600
+++ libc/sysdeps/x86_64/memcmp.S	2006-05-18 14:43:07.611277000 -0500
@@ -0,0 +1,328 @@
+# (c) 2002 Advanced Micro Devices, Inc.
+# YOUR USE OF THIS CODE IS SUBJECT TO THE TERMS
+# AND CONDITIONS OF THE GNU LESSER GENERAL PUBLIC
+# LICENSE FOUND IN THE "README" FILE THAT IS
+# INCLUDED WITH THIS FILE
+
+#include "sysdep.h"
+#if defined PIC && defined SHARED
+# include <rtld-global-offsets.h>
+#endif
+
+#if defined PIC && defined SHARED
+	.globl _rtld_local_ro
+        .hidden _rtld_local_ro
+        .set    _rtld_local_ro,_rtld_global_ro
+#endif
+
+        .text
+
+ENTRY   (memcmp)                        # (const void *, const void*, size_t)
+
+L(try1):				# up to 8B
+        cmp     $8, %rdx
+        jae     L(1after)
+
+L(1):                                	# 1-byte
+        test    %rdx, %rdx
+        mov     $0, %eax
+        jz      L(exit)
+
+L(1loop):
+        movzbl  (%rdi), %eax
+        movzbl  (%rsi), %ecx
+        sub     %ecx, %eax
+        jnz     L(exit)
+
+        dec     %rdx
+
+        lea     1 (%rdi), %rdi
+        lea     1 (%rsi), %rsi
+
+        jnz     L(1loop)
+
+L(exit):
+        rep
+        ret
+
+        .p2align 4
+
+L(1after):
+
+L(8try):			# up to 32B
+        cmp     $32, %rdx
+        jae     L(8after)
+
+L(8):                        	# 8-byte
+        mov     %edx, %ecx
+        shr     $3, %ecx
+        jz      L(1)
+
+        .p2align 4
+
+L(8loop):
+        mov     (%rsi), %rax
+        cmp     (%rdi), %rax
+        jne     L(1)
+
+        sub     $8, %rdx
+        dec     %ecx
+
+        lea     8 (%rsi), %rsi
+        lea     8 (%rdi), %rdi
+
+        jnz     L(8loop)
+
+L(8skip):
+        and     $7, %edx
+        jnz     L(1)
+
+        xor     %eax, %eax
+        ret
+
+        .p2align 4
+
+L(8after):
+
+L(32try):			# up to 2KB
+        cmp     $2048, %rdx
+        ja      L(32after)
+
+L(32):                          # 32-byte
+        mov     %edx, %ecx
+        shr     $5, %ecx
+        jz      L(8)
+
+        .p2align 4
+
+L(32loop):
+        mov        (%rsi), %rax
+        mov      8 (%rsi),  %r8
+        mov     16 (%rsi),  %r9
+        mov     24 (%rsi), %r10
+        sub        (%rdi), %rax
+        sub      8 (%rdi),  %r8
+        sub     16 (%rdi),  %r9
+        sub     24 (%rdi), %r10
+
+        or      %rax,  %r8
+        or       %r9, %r10
+        or       %r8, %r10
+        jnz     L(8)
+
+        sub     $32, %rdx
+        dec     %ecx
+
+        lea     32 (%rsi), %rsi
+        lea     32 (%rdi), %rdi
+
+        jnz     L(32loop)
+
+L(32skip):
+        and     $31, %edx
+        jnz     L(8)
+
+        xor     %eax, %eax
+        ret
+
+        .p2align 4
+
+L(32after):
+
+L(srctry):
+        mov     %esi, %r8d      # align by source
+
+        and     $7, %r8d
+        jz      L(srcafter)     # not unaligned
+
+L(src):                         # align
+        lea     -8 (%r8, %rdx), %rdx
+        sub     $8, %r8d
+
+#       .p2align 4
+
+L(srcloop):
+        movzbl  (%rdi), %eax
+        movzbl  (%rsi), %ecx
+        sub     %ecx, %eax
+        jnz     L(exit)
+
+        inc     %r8d
+
+        lea     1 (%rdi), %rdi
+        lea     1 (%rsi), %rsi
+
+        jnz     L(srcloop)
+
+        .p2align 4
+
+L(srcafter):
+
+L(64try):			# up to 1/2 L1
+#ifdef PIC
+# ifdef SHARED
+        mov     _rtld_local_ro@GOTPCREL (%rip), %rcx
+	mov	RTLD_GLOBAL_DL_CACHE1SIZEHALF (%rcx), %rcx
+# else
+	mov     _dl_cache1sizehalf@GOTPCREL (%rip), %rcx
+	mov     (%rcx), %rcx
+# endif
+#else
+        mov     _dl_cache1sizehalf, %rcx
+#endif
+        cmp	%rdx, %rcx
+        cmova   %rdx, %rcx
+
+L(64):                          # 64-byte
+        shr     $6, %rcx
+        jz      L(32)
+
+        .p2align 4
+
+L(64loop):
+        mov        (%rsi), %rax
+        mov      8 (%rsi),  %r8
+        sub        (%rdi), %rax
+        sub      8 (%rdi),  %r8
+        or      %r8,  %rax
+
+        mov     16 (%rsi),  %r9
+        mov     24 (%rsi), %r10
+        sub     16 (%rdi),  %r9
+        sub     24 (%rdi), %r10
+        or      %r10, %r9
+
+        or      %r9,  %rax
+        jnz     L(32)
+
+        mov     32 (%rsi), %rax
+        mov     40 (%rsi),  %r8
+        sub     32 (%rdi), %rax
+        sub     40 (%rdi),  %r8
+        or      %r8,  %rax
+
+        mov     48 (%rsi),  %r9
+        mov     56 (%rsi), %r10
+        sub     48 (%rdi),  %r9
+        sub     56 (%rdi), %r10
+        or      %r10, %r9
+
+        or      %r9,  %rax
+        jnz    	L(32)
+
+        lea     64 (%rsi), %rsi
+        lea     64 (%rdi), %rdi
+
+        sub     $64, %rdx
+        dec     %rcx
+        jnz     L(64loop)
+
+#       .p2align 4
+
+L(64skip):
+        cmp     $2048, %rdx
+        ja     	L(64after)
+
+        test    %edx, %edx
+        jnz     L(32)
+
+        xor     %eax, %eax
+        ret
+
+        .p2align 4
+
+L(64after):
+
+L(128try):
+
+L(128):                              # 128-byte
+        mov     %rdx, %rcx
+        shr     $7, %rcx
+        jz      L(128skip)
+
+        .p2align 4
+
+L(128loop):
+        prefetcht0 512 (%rsi)
+        prefetcht0 512 (%rdi)
+
+        mov        (%rsi), %rax
+        mov      8 (%rsi), %r8
+        sub        (%rdi), %rax
+        sub      8 (%rdi), %r8
+        mov     16 (%rsi), %r9
+        mov     24 (%rsi), %r10
+        sub     16 (%rdi), %r9
+        sub     24 (%rdi), %r10
+
+        or       %r8, %rax
+        or       %r9, %r10
+        or      %r10, %rax
+
+        mov     32 (%rsi), %r8
+        mov     40 (%rsi), %r9
+        sub     32 (%rdi), %r8
+        sub     40 (%rdi), %r9
+        mov     48 (%rsi), %r10
+        mov     56 (%rsi), %r11
+        sub     48 (%rdi), %r10
+        sub     56 (%rdi), %r11
+
+        or       %r9, %r8
+        or      %r11, %r10
+        or      %r10, %r8
+
+        or      %r8, %rax
+        jnz     L(32)
+
+        prefetcht0 576 (%rsi)
+        prefetcht0 576 (%rdi)
+
+        mov      64 (%rsi), %rax
+        mov      72 (%rsi), %r8
+        sub      64 (%rdi), %rax
+        sub      72 (%rdi), %r8
+        mov      80 (%rsi), %r9
+        mov      88 (%rsi), %r10
+        sub      80 (%rdi), %r9
+        sub      88 (%rdi), %r10
+
+        or       %r8, %rax
+        or       %r9, %r10
+        or      %r10, %rax
+
+        mov      96 (%rsi), %r8
+        mov     104 (%rsi), %r9
+        sub      96 (%rdi), %r8
+        sub     104 (%rdi), %r9
+        mov     112 (%rsi), %r10
+        mov     120 (%rsi), %r11
+        sub     112 (%rdi), %r10
+        sub     120 (%rdi), %r11
+
+        or       %r9, %r8
+        or      %r11, %r10
+        or      %r10, %r8
+
+        or      %r8, %rax
+        jnz     L(32)
+
+        sub     $128, %rdx
+        dec     %rcx
+
+        lea     128 (%rsi), %rsi
+        lea     128 (%rdi), %rdi
+
+        jnz     L(128loop)
+
+L(128skip):
+        and     $127, %edx
+        jnz     L(32)
+
+        xor     %eax, %eax
+        ret
+
+END     (memcmp)
+
+#undef bcmp
+weak_alias (memcmp, bcmp)
diff -Npruw -x CVS -x vssver.scc -x powerpc -x sync_file_range.c libc/sysdeps/x86_64/memset.S libc/sysdeps/x86_64/memset.S
--- libc/sysdeps/x86_64/memset.S	2005-03-31 04:00:13.000000000 -0600
+++ libc/sysdeps/x86_64/memset.S	2006-05-15 11:38:13.737756000 -0500
@@ -1,145 +1,322 @@
-/* memset/bzero -- set memory area to CH/0
-   Optimized version for x86-64.
-   Copyright (C) 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Andreas Jaeger <aj@suse.de>.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
+# (c) 2002 Advanced Micro Devices, Inc.
+# YOUR USE OF THIS CODE IS SUBJECT TO THE TERMS
+# AND CONDITIONS OF THE GNU LESSER GENERAL PUBLIC
+# LICENSE FOUND IN THE "README" FILE THAT IS
+# INCLUDED WITH THIS FILE
 
 #include <sysdep.h>
 #include "asm-syntax.h"
 #include "bp-sym.h"
 #include "bp-asm.h"
+#if defined PIC && defined SHARED
+# include <rtld-global-offsets.h>
+#endif
 
-/* BEWARE: `#ifdef memset' means that memset is redefined as `bzero' */
-#define BZERO_P (defined memset)
-
-/* This is somehow experimental and could made dependend on the cache
-   size.  */
-#define LARGE $120000
+#if defined PIC && defined SHARED
+	.globl _rtld_local_ro
+        .hidden _rtld_local_ro
+        .set    _rtld_local_ro,_rtld_global_ro
+#endif
 
         .text
-#if !BZERO_P && defined PIC && !defined NOT_IN_libc
+#if !defined USE_AS_BZERO && defined PIC && !defined NOT_IN_libc
 ENTRY (__memset_chk)
 	cmpq	%rdx, %rcx
 	jb	HIDDEN_JUMPTARGET (__chk_fail)
 END (__memset_chk)
 #endif
-ENTRY (memset)
-#if BZERO_P
-	mov	%rsi,%rdx	/* Adjust parameter.  */
-	xorl	%esi,%esi	/* Fill with 0s.  */
-#endif
-	cmp	$0x7,%rdx	/* Check for small length.  */
-	mov	%rdi,%rcx	/* Save ptr as return value.  */
-	jbe	7f
 
-#if BZERO_P
-	mov	%rsi,%r8	/* Just copy 0.  */
+ENTRY (memset)                		# (void *, const void*, size_t)
+
+#ifdef USE_AS_BZERO
+        mov     %rsi, %rdx		# memset doubles as bzero
+        xorl    %esi, %esi
+#else
+	mov	$0x0101010101010101, %rcx # memset is itself
+        movzx   %sil, %rsi
+        imul    %rcx, %rsi		# replicate 8 times
+#endif
+
+L(try1):				# up to 64B
+        cmp     $64, %rdx
+        mov     %rdi, %rax		# return memory block address (even for bzero ())
+        jae	L(1after)
+
+L(1):                                	# 1-byte loop
+        test    $1, %dl
+        jz      L(1a)
+
+        mov     %sil, (%rdi)
+        inc	%rdi
+
+L(1a):
+        test    $2, %dl
+        jz      L(1b)
+
+        mov     %si, (%rdi)
+        add	$2, %rdi
+
+L(1b):
+        test    $4, %dl
+        jz      L(1c)
+
+        mov     %esi, (%rdi)
+	add	$4, %rdi
+
+L(1c):
+        test    $8, %dl
+        jz      L(1d)
+
+        mov     %rsi, (%rdi)
+	add	$8, %rdi
+
+L(1d):
+        test    $16, %dl
+        jz      L(1e)
+
+        mov     %rsi,   (%rdi)
+        mov     %rsi, 8 (%rdi)
+	add	$16, %rdi
+
+L(1e):
+        test    $32, %dl
+        jz      L(1f)
+
+        mov     %rsi,    (%rdi)
+        mov     %rsi,  8 (%rdi)
+        mov     %rsi, 16 (%rdi)
+        mov     %rsi, 24 (%rdi)
+#	add	$32, %rdi
+
+L(1f):
+
+L(exit):
+        rep
+        ret
+
+        .p2align 4
+
+L(1after):
+
+L(32try):				# up to 512B
+	cmp	$512, %rdx
+        ja	L(32after)
+
+L(32):                               	# 32-byte loop
+        mov     %edx, %ecx
+        shr     $5, %ecx
+        jz      L(32skip)
+
+        .p2align 4
+
+L(32loop):
+        dec     %ecx
+
+        mov     %rsi,    (%rdi)
+        mov     %rsi,  8 (%rdi)
+        mov     %rsi, 16 (%rdi)
+        mov     %rsi, 24 (%rdi)
+
+        lea     32 (%rdi), %rdi
+
+        jz      L(32skip)
+
+        dec     %ecx
+
+        mov     %rsi,    (%rdi)
+        mov     %rsi,  8 (%rdi)
+        mov     %rsi, 16 (%rdi)
+        mov     %rsi, 24 (%rdi)
+
+        lea     32 (%rdi), %rdi
+
+        jnz     L(32loop)
+
+        .p2align 4
+
+L(32skip):
+        and     $31, %edx		# check for left overs
+        jnz     L(1)
+
+        rep
+        ret
+
+        .p2align 4
+
+L(32after):
+
+L(aligntry):
+        mov     %edi, %ecx              # align by destination
+
+        and     $7, %ecx                # skip if already aligned
+        jz      L(alignafter)
+
+L(align):                            	# align loop
+        lea     -8 (%rcx, %rdx), %rdx
+        sub     $8, %ecx
+
+        .p2align 4
+
+L(alignloop):
+        inc     %ecx
+
+        mov     %sil, (%rdi)
+        lea     1 (%rdi), %rdi
+
+        jnz     L(alignloop)
+
+        .p2align 4
+
+L(alignafter):
+
+# For MP System half cache size is better,
+# for UP full cache size is better.
+# Use half cache size only.
+L(fasttry):				# between 2KB and 1/2 L2
+#ifdef PIC
+# ifdef SHARED
+        mov     _rtld_local_ro@GOTPCREL (%rip), %r8
+	mov	RTLD_GLOBAL_DL_CACHE2SIZEHALF (%r8), %r8
 #else
-	/* Populate 8 bit data to full 64-bit.  */
-	movabs	$0x0101010101010101,%r8
-	movzbl	%sil,%eax
-	imul	%rax,%r8
-#endif
-	test	$0x7,%edi	/* Check for alignment.  */
-	je	2f
-
-	.p2align 4
-1:	/* Align ptr to 8 byte.  */
-	mov	%sil,(%rcx)
-	dec	%rdx
-	inc	%rcx
-	test	$0x7,%ecx
-	jne	1b
-
-2:	/* Check for really large regions.  */
-	mov	%rdx,%rax
-	shr	$0x6,%rax
-	je	4f
-	cmp	LARGE, %rdx
-	jae	11f
-
-	.p2align 4
-3:	/* Copy 64 bytes.  */
-	mov	%r8,(%rcx)
-	mov	%r8,0x8(%rcx)
-	mov	%r8,0x10(%rcx)
-	mov	%r8,0x18(%rcx)
-	mov	%r8,0x20(%rcx)
-	mov	%r8,0x28(%rcx)
-	mov	%r8,0x30(%rcx)
-	mov	%r8,0x38(%rcx)
-	add	$0x40,%rcx
-	dec	%rax
-	jne	3b
-
-4:	/* Copy final bytes.  */
-	and	$0x3f,%edx
-	mov	%rdx,%rax
-	shr	$0x3,%rax
-	je	6f
-
-5:	/* First in chunks of 8 bytes.  */
-	mov	%r8,(%rcx)
-	add	$0x8,%rcx
-	dec	%rax
-	jne	5b
-6:
-	and	$0x7,%edx
-7:
-	test	%rdx,%rdx
-	je	9f
-8:	/* And finally as bytes (up to 7).  */
-	mov	%sil,(%rcx)
-	inc	%rcx
-	dec	%rdx
-	jne	8b
-9:
-#if BZERO_P
-	nop
+        mov     _dl_cache2sizehalf@GOTPCREL (%rip), %r8
+	mov	(%r8), %r8
+# endif
 #else
-	/* Load result (only if used as memset).  */
-	mov	%rdi,%rax	/* start address of destination is result */
+        mov     _dl_cache2sizehalf, %r8
 #endif
-	retq
+        cmp     %rdx, %r8
+        cmova   %rdx, %r8
+
+	cmp	$2048, %rdx		# this is slow for some block sizes
+	jb	L(64)
+
+L(fast):				# microcode loop
+	mov	%r8, %rcx
+	and	$-8, %r8
+	shr	$3, %rcx
+
+	xchg	%rax, %rsi
+
+	rep
+	stosq
+
+	xchg	%rax, %rsi
+
+L(fastskip):
+	sub	%r8, %rdx		# check for more
+	ja	L(64after)
+
+	and	$7, %edx		# check for left overs
+	jnz	L(1)
+
+	rep
+	ret
 
 	.p2align 4
-11:	/* Copy 64 bytes without polluting the cache.  */
-	/* We could use	movntdq    %xmm0,(%rcx) here to further
-	   speed up for large cases but let's not use XMM registers.  */
-	movnti	%r8,(%rcx)
-	movnti  %r8,0x8(%rcx)
-	movnti  %r8,0x10(%rcx)
-	movnti  %r8,0x18(%rcx)
-	movnti  %r8,0x20(%rcx)
-	movnti  %r8,0x28(%rcx)
-	movnti  %r8,0x30(%rcx)
-	movnti  %r8,0x38(%rcx)
-	add	$0x40,%rcx
-	dec	%rax
-	jne	11b
-	jmp	4b
+
+L(fastafter):
+
+L(64try):				# up to 2KB
+
+L(64):                               	# 64-byte loop
+        mov     %r8, %rcx
+        and     $-64, %r8
+        shr     $6, %rcx
+
+        dec     %rcx                    # this iteration starts the prefetcher sooner
+
+        mov     %rsi,    (%rdi)
+        mov     %rsi,  8 (%rdi)
+        mov     %rsi, 16 (%rdi)
+        mov     %rsi, 24 (%rdi)
+        mov     %rsi, 32 (%rdi)
+        mov     %rsi, 40 (%rdi)
+        mov     %rsi, 48 (%rdi)
+        mov     %rsi, 56 (%rdi)
+
+        lea     64 (%rdi), %rdi
+
+        .p2align 4
+
+L(64loop):
+        dec     %rcx
+
+        mov     %rsi,    (%rdi)
+        mov     %rsi,  8 (%rdi)
+        mov     %rsi, 16 (%rdi)
+        mov     %rsi, 24 (%rdi)
+        mov     %rsi, 32 (%rdi)
+        mov     %rsi, 40 (%rdi)
+        mov     %rsi, 48 (%rdi)
+        mov     %rsi, 56 (%rdi)
+
+        lea     64 (%rdi), %rdi
+
+        jnz     L(64loop)
+
+L(64skip):
+        sub     %r8, %rdx		# check for more
+        ja      L(64after)
+
+	and     $63, %edx		# check for left overs
+	jnz     L(32)
+
+        rep
+        ret
+
+        .p2align 4
+
+L(64after):
+
+L(NTtry):
+
+L(NT):                               	# 128-byte NT loop
+        mov     %rdx, %rcx
+        shr     $7, %rcx
+        jz      L(NTskip)
+
+        .p2align 4
+
+L(NTloop):                  		# on an MP system it would be better to prefetchnta 320 (%rdi) and 384 (%rdi) here, but not so on an 1P system
+        dec     %rcx
+
+        movnti  %rsi,     (%rdi)
+        movnti  %rsi,   8 (%rdi)
+        movnti  %rsi,  16 (%rdi)
+        movnti  %rsi,  24 (%rdi)
+        movnti  %rsi,  32 (%rdi)
+        movnti  %rsi,  40 (%rdi)
+        movnti  %rsi,  48 (%rdi)
+        movnti  %rsi,  56 (%rdi)
+        movnti  %rsi,  64 (%rdi)
+        movnti  %rsi,  72 (%rdi)
+        movnti  %rsi,  80 (%rdi)
+        movnti  %rsi,  88 (%rdi)
+        movnti  %rsi,  96 (%rdi)
+        movnti  %rsi, 104 (%rdi)
+        movnti  %rsi, 112 (%rdi)
+        movnti  %rsi, 120 (%rdi)
+
+        lea     128 (%rdi), %rdi
+
+        jnz     L(NTloop)
+
+        mfence				# serialize memory operations
+
+L(NTskip):
+        and     $127, %edx		# check for left overs
+        jnz     L(32)
+
+        rep
+        ret
 
 END (memset)
-#if !BZERO_P
+
+#ifndef USE_AS_BZERO
 libc_hidden_builtin_def (memset)
 #endif
 
-#if !BZERO_P && defined PIC && !defined NOT_IN_libc
+#if !defined USE_AS_BZERO && defined PIC && !defined NOT_IN_libc
 strong_alias (__memset_chk, __memset_zero_constant_len_parameter)
 	.section .gnu.warning.__memset_zero_constant_len_parameter
 	.string "memset used with constant zero length parameter; this could be due to transposed parameters"
diff -Npruw -x CVS -x vssver.scc -x powerpc -x sync_file_range.c libc/sysdeps/x86_64/stpcpy.S libc/sysdeps/x86_64/stpcpy.S
--- libc/sysdeps/x86_64/stpcpy.S	2004-05-28 01:39:37.000000000 -0500
+++ libc/sysdeps/x86_64/stpcpy.S	2006-05-05 15:24:41.775991000 -0500
@@ -1,5 +1,5 @@
 #define USE_AS_STPCPY
-#define STRCPY __stpcpy
+#define strcpy __stpcpy
 
 #include <sysdeps/x86_64/strcpy.S>
 
diff -Npruw -x CVS -x vssver.scc -x powerpc -x sync_file_range.c libc/sysdeps/x86_64/stpncpy.S libc/sysdeps/x86_64/stpncpy.S
--- libc/sysdeps/x86_64/stpncpy.S	1969-12-31 18:00:00.000000000 -0600
+++ libc/sysdeps/x86_64/stpncpy.S	2006-05-05 15:24:50.748541000 -0500
@@ -0,0 +1,9 @@
+#define USE_AS_STRNCPY
+#define USE_AS_STPCPY
+#define strcpy __stpncpy
+
+#include <sysdeps/x86_64/strcpy.S>
+
+weak_alias (__stpncpy, stpncpy)
+libc_hidden_def (__stpncpy)
+libc_hidden_builtin_def (stpncpy)
diff -Npruw -x CVS -x vssver.scc -x powerpc -x sync_file_range.c libc/sysdeps/x86_64/strcpy.S libc/sysdeps/x86_64/strcpy.S
--- libc/sysdeps/x86_64/strcpy.S	2003-04-29 17:47:18.000000000 -0500
+++ libc/sysdeps/x86_64/strcpy.S	2006-05-19 13:41:31.281326000 -0500
@@ -1,159 +1,1141 @@
-/* strcpy/stpcpy implementation for x86-64.
-   Copyright (C) 2002 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Andreas Jaeger <aj@suse.de>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <sysdep.h>
-#include "asm-syntax.h"
-#include "bp-sym.h"
-#include "bp-asm.h"
+# (c) 2002 Advanced Micro Devices, Inc.
+# YOUR USE OF THIS CODE IS SUBJECT TO THE TERMS
+# AND CONDITIONS OF THE GNU LESSER GENERAL PUBLIC
+# LICENSE FOUND IN THE "README" FILE THAT IS
+# INCLUDED WITH THIS FILE
+
+#include "sysdep.h"
+#if defined PIC && defined SHARED
+# include <rtld-global-offsets.h>
+#endif
 
-#ifndef USE_AS_STPCPY
-# define STRCPY strcpy
+#if defined PIC && defined SHARED
+	.globl _rtld_local_ro
+        .hidden _rtld_local_ro
+        .set    _rtld_local_ro,_rtld_global_ro
 #endif
 
 	.text
-ENTRY (BP_SYM (STRCPY))
-	movq %rsi, %rcx		/* Source register. */
-	andl $7, %ecx		/* mask alignment bits */
-	movq %rdi, %rdx		/* Duplicate destination pointer.  */
-
-	jz 5f			/* aligned => start loop */
-
-	neg %ecx		/* We need to align to 8 bytes.  */
-	addl $8,%ecx
-	/* Search the first bytes directly.  */
-0:
-	movb	(%rsi), %al	/* Fetch a byte */
-	testb	%al, %al	/* Is it NUL? */
-	movb	%al, (%rdx)	/* Store it */
-	jz	4f		/* If it was NUL, done! */
-	incq	%rsi
-	incq	%rdx
-	decl	%ecx
-	jnz	0b
-
-5:
-	movq $0xfefefefefefefeff,%r8
-
-	/* Now the sources is aligned.  Unfortunatly we cannot force
-	   to have both source and destination aligned, so ignore the
-	   alignment of the destination.  */
+
+ENTRY   (strcpy)                        # (char *, const char *)
+
+#ifdef USE_AS_STRNCPY			// (char *, const char *, size_t)
+	test	%rdx, %rdx
+	mov	%rdx, %r11
+	jz	L(exit)			# early exit
+#endif
+
+        xor     %edx, %edx
+
+L(aligntry):				# between 0 and 7 bytes
+        mov     %rsi, %r8		# align by source
+        and     $7, %r8
+	jz	L(alignafter)
+
+L(align):				# 8-byte align
+        sub     $8, %r8
+
 	.p2align 4
-1:
-	/* 1st unroll.  */
-	movq	(%rsi), %rax	/* Read double word (8 bytes).  */
-	addq	$8, %rsi	/* Adjust pointer for next word.  */
-	movq	%rax, %r9	/* Save a copy for NUL finding.  */
-	addq	%r8, %r9	/* add the magic value to the word.  We get
-				   carry bits reported for each byte which
-				   is *not* 0 */
-	jnc	3f		/* highest byte is NUL => return pointer */
-	xorq	%rax, %r9	/* (word+magic)^word */
-	orq	%r8, %r9	/* set all non-carry bits */
-	incq	%r9		/* add 1: if one carry bit was *not* set
-				   the addition will not result in 0.  */
-
-	jnz	3f		/* found NUL => return pointer */
-
-	movq	%rax, (%rdx)	/* Write value to destination.  */
-	addq	$8, %rdx	/* Adjust pointer.  */
-
-	/* 2nd unroll.  */
-	movq	(%rsi), %rax	/* Read double word (8 bytes).  */
-	addq	$8, %rsi	/* Adjust pointer for next word.  */
-	movq	%rax, %r9	/* Save a copy for NUL finding.  */
-	addq	%r8, %r9	/* add the magic value to the word.  We get
-				   carry bits reported for each byte which
-				   is *not* 0 */
-	jnc	3f		/* highest byte is NUL => return pointer */
-	xorq	%rax, %r9	/* (word+magic)^word */
-	orq	%r8, %r9	/* set all non-carry bits */
-	incq	%r9		/* add 1: if one carry bit was *not* set
-				   the addition will not result in 0.  */
-
-	jnz	3f		/* found NUL => return pointer */
-
-	movq	%rax, (%rdx)	/* Write value to destination.  */
-	addq	$8, %rdx	/* Adjust pointer.  */
-
-	/* 3rd unroll.  */
-	movq	(%rsi), %rax	/* Read double word (8 bytes).  */
-	addq	$8, %rsi	/* Adjust pointer for next word.  */
-	movq	%rax, %r9	/* Save a copy for NUL finding.  */
-	addq	%r8, %r9	/* add the magic value to the word.  We get
-				   carry bits reported for each byte which
-				   is *not* 0 */
-	jnc	3f		/* highest byte is NUL => return pointer */
-	xorq	%rax, %r9	/* (word+magic)^word */
-	orq	%r8, %r9	/* set all non-carry bits */
-	incq	%r9		/* add 1: if one carry bit was *not* set
-				   the addition will not result in 0.  */
-
-	jnz	3f		/* found NUL => return pointer */
-
-	movq	%rax, (%rdx)	/* Write value to destination.  */
-	addq	$8, %rdx	/* Adjust pointer.  */
-
-	/* 4th unroll.  */
-	movq	(%rsi), %rax	/* Read double word (8 bytes).  */
-	addq	$8, %rsi	/* Adjust pointer for next word.  */
-	movq	%rax, %r9	/* Save a copy for NUL finding.  */
-	addq	%r8, %r9	/* add the magic value to the word.  We get
-				   carry bits reported for each byte which
-				   is *not* 0 */
-	jnc	3f		/* highest byte is NUL => return pointer */
-	xorq	%rax, %r9	/* (word+magic)^word */
-	orq	%r8, %r9	/* set all non-carry bits */
-	incq	%r9		/* add 1: if one carry bit was *not* set
-				   the addition will not result in 0.  */
-
-	jnz	3f		/* found NUL => return pointer */
-
-	movq	%rax, (%rdx)	/* Write value to destination.  */
-	addq	$8, %rdx	/* Adjust pointer.  */
-	jmp	1b		/* Next iteration.  */
 
-	/* Do the last few bytes. %rax contains the value to write.
-	   The loop is unrolled twice.  */
+L(alignloop):
+        movzbl	(%rsi, %rdx), %eax
+        test    %al, %al                # check if character a NUL
+        mov     %al, (%rdi, %rdx)
+        jz      L(exit)
+
+        inc     %edx
+
+#ifdef USE_AS_STRNCPY
+	dec	%r11
+	jz	L(exit)
+#endif
+
+        inc     %r8
+        jnz     L(alignloop)
+
+	.p2align 4,, 7
+
+L(alignafter):
+
+L(8try):				# up to 64 bytes
+        mov     $0xfefefefefefefeff, %rcx
+
+L(8):                               	# 8-byte loop
+
+L(8loop):
+#ifdef USE_AS_STRNCPY
+	sub	$8, %r11
+	jbe	L(tail)
+#endif
+
+        mov     (%rsi, %rdx), %rax
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        jnc	L(tail)			# sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        inc	%r8			# sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+        add     $8, %edx
+
+L(8after):				# up to 64 bytes
+
+L(64try):				# up to 1/2 L1
+#ifdef PIC
+# ifdef SHARED
+        mov     _rtld_local_ro@GOTPCREL (%rip), %r9
+	mov	RTLD_GLOBAL_DL_CACHE1SIZEHALF (%r9), %r9
+# else
+        mov     _dl_cache1sizehalf@GOTPCREL (%rip), %r9
+	mov	(%r9), %r9
+# endif
+#else
+        mov     _dl_cache1sizehalf, %r9
+#endif
+
+L(64):					# 64-byte loop
+
 	.p2align 4
-3:
-	/* Note that stpcpy needs to return with the value of the NUL
-	   byte.  */
-	movb	%al, (%rdx)	/* 1st byte.  */
-	testb	%al, %al	/* Is it NUL.  */
-	jz	4f		/* yes, finish.  */
-	incq	%rdx		/* Increment destination.  */
-	movb	%ah, (%rdx)	/* 2nd byte.  */
-	testb	%ah, %ah	/* Is it NUL?.  */
-	jz	4f		/* yes, finish.  */
-	incq	%rdx		/* Increment destination.  */
-	shrq	$16, %rax	/* Shift...  */
-	jmp	3b		/* and look at next two bytes in %rax.  */
 
-4:
+L(64loop):
+#ifdef USE_AS_STRNCPY
+	sub	$8, %r11
+	jbe	L(tail)
+#endif
+
+        mov     (%rsi, %rdx), %rax
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+        add     $8, %edx
+
+        cmp     %r9, %rdx
+        jbe     L(64loop)
+
+L(64after):				# up to 1/2 L1
+
+L(pretry):				# up to 1/2 L2
+#ifdef PIC
+# ifdef SHARED
+	mov     _rtld_local_ro@GOTPCREL (%rip), %r9
+	cmpl	$0, RTLD_GLOBAL_DL_PREFETCHW (%r9)
+	mov	RTLD_GLOBAL_DL_CACHE2SIZEHALF (%r9), %r9
+# else
+	mov     _dl_prefetchw@GOTPCREL (%rip), %r9
+	cmpl	$0, (%r9)
+	mov     _dl_cache2sizehalf@GOTPCREL (%rip), %r9
+	mov	(%r9), %r9
+# endif
+#else
+	cmpl	$0, _dl_prefetchw
+        mov     _dl_cache2sizehalf, %r9
+#endif
+	jz	L(preloop)		# check for availability of PREFETCHW
+
+        .p2align 4
+
+L(prewloop):				# 64-byte with prefetching to state M
+#ifdef USE_AS_STRNCPY
+	sub	$8, %r11
+	jbe	L(tail)
+#endif
+
+        mov     (%rsi, %rdx), %rax
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+        prefetchw 512 + 8 (%rdi, %rdx)
+        prefetcht0 512 + 8 (%rsi, %rdx)
+
+        add     $8, %edx
+
+        cmp     %r9, %rdx
+        jb	L(prewloop)
+	jmp	L(preafter)
+
+L(prewafter):				# up to 1/2 L2
+
+        .p2align 4
+
+L(preloop):				# 64-byte with prefetching to state E
+#ifdef USE_AS_STRNCPY
+	sub	$8, %r11
+	jbe	L(tail)
+#endif
+
+        mov     (%rsi, %rdx), %rax
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %edx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %edx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(tail)
+
+        mov     %rax, (%rdi, %rdx)
+
+        prefetcht0 512 + 8 (%rdi, %rdx)
+        prefetcht0 512 + 8 (%rsi, %rdx)
+
+        add     $8, %edx
+
+        cmp     %r9, %rdx
+        jb	L(preloop)
+
+        .p2align 4
+
+L(preafter):				# up to 1/2 of L2
+
+L(NTtry):
+	mfence
+
+L(NT):					# 64-byte NT
+
+        .p2align 4
+
+L(NTloop):
+#ifdef USE_AS_STRNCPY
+	sub	$8, %r11
+	jbe	L(tail)
+#endif
+
+        mov     (%rsi, %rdx), %rax
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %rdx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %rdx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %rdx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %rdx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %rdx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %rdx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %rdx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %rdx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %rdx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %rdx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %rdx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %rdx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+#ifdef USE_AS_STRNCPY
+        add     $8, %rdx
+
+	sub	$8, %r11
+	jbe	L(tail)
+
+        mov     (%rsi, %rdx), %rax
+#else
+        mov     8 (%rsi, %rdx), %rax
+        add     $8, %rdx
+#endif
+
+        mov     %rcx, %r8
+        add     %rax, %r8
+        sbb     %r10, %r10
+
+        xor     %rax, %r8
+        or      %rcx, %r8
+        sub     %r10, %r8
+        jnz     L(NTtail)
+
+        movnti  %rax, (%rdi, %rdx)
+
+	prefetchnta 768 + 8 (%rsi, %rdx)
+
+        add     $8, %rdx
+        jmp     L(NTloop)
+
+        .p2align 4
+
+L(NTtail):
+	mfence				# serialize memory operations
+
+        .p2align 4
+
+L(NTafter):
+
+L(tailtry):
+
+L(tail):				# 1-byte tail
+#ifdef USE_AS_STRNCPY
+	add	$8, %r11
+	jz	L(exit)
+#endif
+
+        .p2align 4
+
+L(tailloop):
+        movzbl	(%rsi, %rdx), %eax
+        test    %al, %al
+        mov     %al, (%rdi, %rdx)
+        jz      L(exit)
+
+	inc     %rdx
+
+#ifdef USE_AS_STRNCPY
+	dec	%r11
+	jz	L(exit)
+#endif
+        jmp     L(tailloop)
+
+        .p2align 4
+
+L(tailafter):
+
+L(exit):
+#ifdef USE_AS_STPCPY
+        lea     (%rdi, %rdx), %rax
+#else
+        mov     %rdi, %rax
+#endif
+
+#ifdef USE_AS_STRNCPY
+	test	%r11, %r11
+	mov	%r11, %rcx
+	jnz	2f
+
+	rep
+        ret
+
+        .p2align 4
+
+2:
 #ifdef USE_AS_STPCPY
-	movq	%rdx, %rax	/* Destination is return value.  */
+	mov	%rax, %r8
 #else
-	movq	%rdi, %rax	/* Source is return value.  */
+        mov     %rdi, %r8
+# endif
+
+	xor	%eax, %eax		# bzero () would do too, but usually there are only a handfull of bytes left
+	shr	$3, %rcx
+        lea     (%rdi, %rdx), %rdi
+	jz	3f
+
+	rep	stosq
+
+	and	$7, %r11d
+	jz	1f
+
+        .p2align 4,, 4
+
+3:
+	mov	%al, (%rdi)
+	inc	%rdi
+
+	dec	%r11d
+	jnz	3b
+
+        .p2align 4,, 4
+
+1:
+        mov     %r8, %rax
 #endif
-	retq
-END (BP_SYM (STRCPY))
-#ifndef USE_AS_STPCPY
+        ret
+
+END (strcpy)
+
+#if !defined USE_AS_STPCPY && !defined USE_AS_STRNCPY
 libc_hidden_builtin_def (strcpy)
 #endif
diff -Npruw -x CVS -x vssver.scc -x powerpc -x sync_file_range.c libc/sysdeps/x86_64/strncpy.S libc/sysdeps/x86_64/strncpy.S
--- libc/sysdeps/x86_64/strncpy.S	1969-12-31 18:00:00.000000000 -0600
+++ libc/sysdeps/x86_64/strncpy.S	2006-05-05 15:25:34.559341000 -0500
@@ -0,0 +1,8 @@
+#define USE_AS_STRNCPY
+#define strcpy __strncpy
+
+#include <sysdeps/x86_64/strcpy.S>
+
+weak_alias (__strncpy, strncpy)
+libc_hidden_def (__strncpy)
+libc_hidden_builtin_def (strncpy)
