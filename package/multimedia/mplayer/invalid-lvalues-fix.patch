# --- T2-COPYRIGHT-NOTE-BEGIN ---
# This copyright note is auto-generated by ./scripts/Create-CopyPatch.
# 
# T2 SDE: package/.../mplayer/invalid-lvalues-fix.patch
# Copyright (C) 2006 The T2 SDE Project
# 
# More information can be found in the files COPYING and README.
# 
# This patch file is dual-licensed. It is available under the license the
# patched project is licensed under, as long as it is an OpenSource license
# as defined at http://www.opensource.org/ (e.g. BSD, X11) or under the terms
# of the GNU General Public License as published by the Free Software
# Foundation; either version 2 of the License, or (at your option) any later
# version.
# --- T2-COPYRIGHT-NOTE-END ---

--- MPlayer-1.0pre7try2/libavcodec/ppc/fdct_altivec.c.orig	2006-04-20 18:32:26.000000000 +0200
+++ MPlayer-1.0pre7try2/libavcodec/ppc/fdct_altivec.c	2006-04-20 19:39:53.000000000 +0200
@@ -29,7 +29,7 @@
 #define vu8(v)  ((vector unsigned char)(v))
 #define vu16(v) ((vector unsigned short)(v))
 #define vu32(v) ((vector unsigned int)(v))
-
+#define vfl(r)  ((vector float)(r))
 
 #define C1     0.98078525066375732421875000 /* cos(1*PI/16) */
 #define C2     0.92387950420379638671875000 /* cos(2*PI/16) */
@@ -214,8 +214,8 @@
 
     /* setup constants {{{ */
     /* mzero = -0.0 */
-    vu32(mzero) = vec_splat_u32(-1);
-    vu32(mzero) = vec_sl(vu32(mzero), vu32(mzero));
+    mzero = vfl(vec_splat_u32(-1));
+    mzero = vfl(vec_sl(vu32(mzero), vu32(mzero)));
     cp = fdctconsts;
     cnsts0 = vec_ld(0, cp); cp++;
     cnsts1 = vec_ld(0, cp); cp++;
@@ -227,43 +227,43 @@
 #define MERGE_S16(hl,a,b) vec_merge##hl(vs16(a), vs16(b))
 
     bp = (vector signed short*)block;
-    vs16(b00) = vec_ld(0,    bp);
-    vs16(b40) = vec_ld(16*4, bp);
-    vs16(b01) = MERGE_S16(h, b00, b40);
-    vs16(b11) = MERGE_S16(l, b00, b40);
+    b00 = vfl(vec_ld(0,    bp));
+    b40 = vfl(vec_ld(16*4, bp));
+    b01 = vfl(MERGE_S16(h, b00, b40));
+    b11 = vfl(MERGE_S16(l, b00, b40));
     bp++;
-    vs16(b10) = vec_ld(0,    bp);
-    vs16(b50) = vec_ld(16*4, bp);
-    vs16(b21) = MERGE_S16(h, b10, b50);
-    vs16(b31) = MERGE_S16(l, b10, b50);
+    b10 = vfl(vec_ld(0,    bp));
+    b50 = vfl(vec_ld(16*4, bp));
+    b21 = vfl(MERGE_S16(h, b10, b50));
+    b31 = vfl(MERGE_S16(l, b10, b50));
     bp++;
-    vs16(b20) = vec_ld(0,    bp);
-    vs16(b60) = vec_ld(16*4, bp);
-    vs16(b41) = MERGE_S16(h, b20, b60);
-    vs16(b51) = MERGE_S16(l, b20, b60);
+    b20 = vfl(vec_ld(0,    bp));
+    b60 = vfl(vec_ld(16*4, bp));
+    b41 = vfl(MERGE_S16(h, b20, b60));
+    b51 = vfl(MERGE_S16(l, b20, b60));
     bp++;
-    vs16(b30) = vec_ld(0,    bp);
-    vs16(b70) = vec_ld(16*4, bp);
-    vs16(b61) = MERGE_S16(h, b30, b70);
-    vs16(b71) = MERGE_S16(l, b30, b70);
-
-    vs16(x0) = MERGE_S16(h, b01, b41);
-    vs16(x1) = MERGE_S16(l, b01, b41);
-    vs16(x2) = MERGE_S16(h, b11, b51);
-    vs16(x3) = MERGE_S16(l, b11, b51);
-    vs16(x4) = MERGE_S16(h, b21, b61);
-    vs16(x5) = MERGE_S16(l, b21, b61);
-    vs16(x6) = MERGE_S16(h, b31, b71);
-    vs16(x7) = MERGE_S16(l, b31, b71);
-
-    vs16(b00) = MERGE_S16(h, x0, x4);
-    vs16(b10) = MERGE_S16(l, x0, x4);
-    vs16(b20) = MERGE_S16(h, x1, x5);
-    vs16(b30) = MERGE_S16(l, x1, x5);
-    vs16(b40) = MERGE_S16(h, x2, x6);
-    vs16(b50) = MERGE_S16(l, x2, x6);
-    vs16(b60) = MERGE_S16(h, x3, x7);
-    vs16(b70) = MERGE_S16(l, x3, x7);
+    b30 = vfl(vec_ld(0,    bp));
+    b70 = vfl(vec_ld(16*4, bp));
+    b61 = vfl(MERGE_S16(h, b30, b70));
+    b71 = vfl(MERGE_S16(l, b30, b70));
+
+    x0 = vfl(MERGE_S16(h, b01, b41));
+    x1 = vfl(MERGE_S16(l, b01, b41));
+    x2 = vfl(MERGE_S16(h, b11, b51));
+    x3 = vfl(MERGE_S16(l, b11, b51));
+    x4 = vfl(MERGE_S16(h, b21, b61));
+    x5 = vfl(MERGE_S16(l, b21, b61));
+    x6 = vfl(MERGE_S16(h, b31, b71));
+    x7 = vfl(MERGE_S16(l, b31, b71));
+
+    b00 = vfl(MERGE_S16(h, x0, x4));
+    b10 = vfl(MERGE_S16(l, x0, x4));
+    b20 = vfl(MERGE_S16(h, x1, x5));
+    b30 = vfl(MERGE_S16(l, x1, x5));
+    b40 = vfl(MERGE_S16(h, x2, x6));
+    b50 = vfl(MERGE_S16(l, x2, x6));
+    b60 = vfl(MERGE_S16(h, x3, x7));
+    b70 = vfl(MERGE_S16(l, x3, x7));
 
 #undef MERGE_S16
     /* }}} */
@@ -275,32 +275,32 @@
  */
 #if 1
     /* fdct rows {{{ */
-    vs16(x0) = vec_add(vs16(b00), vs16(b70));
-    vs16(x7) = vec_sub(vs16(b00), vs16(b70));
-    vs16(x1) = vec_add(vs16(b10), vs16(b60));
-    vs16(x6) = vec_sub(vs16(b10), vs16(b60));
-    vs16(x2) = vec_add(vs16(b20), vs16(b50));
-    vs16(x5) = vec_sub(vs16(b20), vs16(b50));
-    vs16(x3) = vec_add(vs16(b30), vs16(b40));
-    vs16(x4) = vec_sub(vs16(b30), vs16(b40));
+    x0 = vfl(vec_add(vs16(b00), vs16(b70)));
+    x7 = vfl(vec_sub(vs16(b00), vs16(b70)));
+    x1 = vfl(vec_add(vs16(b10), vs16(b60)));
+    x6 = vfl(vec_sub(vs16(b10), vs16(b60)));
+    x2 = vfl(vec_add(vs16(b20), vs16(b50)));
+    x5 = vfl(vec_sub(vs16(b20), vs16(b50)));
+    x3 = vfl(vec_add(vs16(b30), vs16(b40)));
+    x4 = vfl(vec_sub(vs16(b30), vs16(b40)));
 
-    vs16(b70) = vec_add(vs16(x0), vs16(x3));
-    vs16(b10) = vec_add(vs16(x1), vs16(x2));
+    b70 = vfl(vec_add(vs16(x0), vs16(x3)));
+    b10 = vfl(vec_add(vs16(x1), vs16(x2)));
 
-    vs16(b00) = vec_add(vs16(b70), vs16(b10));
-    vs16(b40) = vec_sub(vs16(b70), vs16(b10));
+    b00 = vfl(vec_add(vs16(b70), vs16(b10)));
+    b40 = vfl(vec_sub(vs16(b70), vs16(b10)));
 
 #define CTF0(n) \
-    vs32(b##n##1) = vec_unpackl(vs16(b##n##0)); \
-    vs32(b##n##0) = vec_unpackh(vs16(b##n##0)); \
+    b##n##1 = vfl(vec_unpackl(vs16(b##n##0))); \
+    b##n##0 = vfl(vec_unpackh(vs16(b##n##0))); \
     b##n##1 = vec_ctf(vs32(b##n##1), 0); \
     b##n##0 = vec_ctf(vs32(b##n##0), 0);
 
     CTF0(0);
     CTF0(4);
 
-    vs16(b20) = vec_sub(vs16(x0), vs16(x3));
-    vs16(b60) = vec_sub(vs16(x1), vs16(x2));
+    b20 = vfl(vec_sub(vs16(x0), vs16(x3)));
+    b60 = vfl(vec_sub(vs16(x1), vs16(x2)));
 
     CTF0(2);
     CTF0(6);
@@ -321,8 +321,8 @@
     b61 = vec_madd(cnst, b61, x1);
 
 #define CTFX(x,b) \
-    vs32(b##0) = vec_unpackh(vs16(x)); \
-    vs32(b##1) = vec_unpackl(vs16(x)); \
+    b##0 = vfl(vec_unpackh(vs16(x))); \
+    b##1 = vfl(vec_unpackl(vs16(x))); \
     b##0 = vec_ctf(vs32(b##0), 0); \
     b##1 = vec_ctf(vs32(b##1), 0); \
 
@@ -400,8 +400,8 @@
 #else
     /* convert to float {{{ */
 #define CTF(n) \
-    vs32(b##n##1) = vec_unpackl(vs16(b##n##0)); \
-    vs32(b##n##0) = vec_unpackh(vs16(b##n##0)); \
+    b##n##1 = vfl(vec_unpackl(vs16(b##n##0))); \
+    b##n##0 = vfl(vec_unpackh(vs16(b##n##0))); \
     b##n##1 = vec_ctf(vs32(b##n##1), 0); \
     b##n##0 = vec_ctf(vs32(b##n##0), 0); \
 
@@ -473,9 +473,9 @@
 #define CTS(n) \
     b##n##0 = vec_round(b##n##0); \
     b##n##1 = vec_round(b##n##1); \
-    vs32(b##n##0) = vec_cts(b##n##0, 0); \
-    vs32(b##n##1) = vec_cts(b##n##1, 0); \
-    vs16(b##n##0) = vec_pack(vs32(b##n##0), vs32(b##n##1)); \
+    b##n##0 = vfl(vec_cts(b##n##0, 0)); \
+    b##n##1 = vfl(vec_cts(b##n##1, 0)); \
+    b##n##0 = vfl(vec_pack(vs32(b##n##0), vs32(b##n##1))); \
     vec_st(vs16(b##n##0), 0, bp);
 
     bp = (vector signed short*)block;
